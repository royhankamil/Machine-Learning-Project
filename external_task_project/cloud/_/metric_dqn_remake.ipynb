{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rizal\\AppData\\Local\\Temp\\ipykernel_14976\\569347725.py:111: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:277.)\n",
      "  states = torch.FloatTensor(states).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: Total Reward: 5292.0\n",
      "Episode 10: Total Reward: 6006.0\n",
      "Episode 20: Total Reward: 6120.0\n",
      "Episode 30: Total Reward: 6190.0\n",
      "Episode 40: Total Reward: 6316.0\n",
      "Episode 50: Total Reward: 6270.0\n",
      "Episode 60: Total Reward: 6366.0\n",
      "Episode 70: Total Reward: 6316.0\n",
      "Episode 80: Total Reward: 6432.0\n",
      "Episode 90: Total Reward: 6304.0\n",
      "Episode 100: Total Reward: 6260.0\n",
      "Episode 110: Total Reward: 6244.0\n",
      "Episode 120: Total Reward: 6236.0\n",
      "Episode 130: Total Reward: 6250.0\n",
      "Episode 140: Total Reward: 6256.0\n",
      "Episode 150: Total Reward: 6354.0\n",
      "Episode 160: Total Reward: 6248.0\n",
      "Episode 170: Total Reward: 6318.0\n",
      "Episode 180: Total Reward: 6356.0\n",
      "Episode 190: Total Reward: 6294.0\n",
      "Episode 200: Total Reward: 6292.0\n",
      "Episode 210: Total Reward: 6284.0\n",
      "Episode 220: Total Reward: 6258.0\n",
      "Episode 230: Total Reward: 6306.0\n",
      "Episode 240: Total Reward: 6290.0\n",
      "Episode 250: Total Reward: 6232.0\n",
      "Episode 260: Total Reward: 6236.0\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "from gym import spaces\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# %%\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"metrics-with-output.csv\")\n",
    "\n",
    "# %%\n",
    "# Preprocessing\n",
    "df['cpu_usage'] = df['cpu_usage'].str.rstrip('%').astype(float) / 100.0\n",
    "df['memory_usage'] = df['memory_usage'].str.rstrip('%').astype(float) / 100.0\n",
    "\n",
    "def clean_bandwidth(value):\n",
    "    if 'GB/s' in value:\n",
    "        return float(value.replace('GB/s', ''))\n",
    "    elif 'MB/s' in value:\n",
    "        return float(value.replace('MB/s', '')) / 1024\n",
    "    else:\n",
    "        return float(value)\n",
    "\n",
    "df['bandwidth_inbound'] = df['bandwidth_inbound'].apply(clean_bandwidth)\n",
    "df['bandwidth_outbound'] = df['bandwidth_outbound'].apply(clean_bandwidth)\n",
    "df['tps'] = df['tps'].str.rstrip(' req/s').astype(float)\n",
    "df['tps_error'] = df['tps_error'].str.rstrip(' req/s').astype(float)\n",
    "df['response_time'] = df['response_time'].replace({' ms': '*1e-3', ' s': '*1'}, regex=True).map(pd.eval).astype(float)\n",
    "\n",
    "# Encode Status\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "df['Status'] = label_encoder.fit_transform(df['Status'])\n",
    "\n",
    "# %%\n",
    "# Pembersihan dan penghapusan data yang tidak dipakai\n",
    "dataset = df.drop(df.columns[[0, 1]], axis=1)\n",
    "\n",
    "# %%\n",
    "# Split data into features and target\n",
    "X = dataset.drop('Status', axis=1)\n",
    "y = dataset['Status']\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# %%\n",
    "# DQN Network\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# %%\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=0.001, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, batch_size=64):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = deque(maxlen=10000)\n",
    "\n",
    "        self.q_network = QNetwork(state_dim, action_dim).to(device)\n",
    "        self.target_network = QNetwork(state_dim, action_dim).to(device)\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        \n",
    "        # Copy the weights to the target network\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return random.choice(range(self.action_dim))\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        q_values = self.q_network(state)\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.FloatTensor(states).to(device)\n",
    "        actions = torch.LongTensor(actions).unsqueeze(1).to(device)\n",
    "        rewards = torch.FloatTensor(rewards).to(device)\n",
    "        next_states = torch.FloatTensor(next_states).to(device)\n",
    "        dones = torch.FloatTensor(dones).to(device)\n",
    "\n",
    "        q_values = self.q_network(states).gather(1, actions).squeeze()\n",
    "        with torch.no_grad():\n",
    "            max_next_q_values = self.target_network(next_states).max(1)[0]\n",
    "            target_q_values = rewards + self.gamma * max_next_q_values * (1 - dones)\n",
    "\n",
    "        loss = self.loss_fn(q_values, target_q_values)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "# %%\n",
    "# Custom Environment\n",
    "class CustomEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(CustomEnv, self).__init__()\n",
    "\n",
    "        # Load data from the dataset\n",
    "        self.df = dataset\n",
    "\n",
    "        # Define action and observation space\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(7,), dtype=np.float64)\n",
    "\n",
    "        self.current_step = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        return self._get_observation()\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= len(self.df) - 1\n",
    "\n",
    "        reward = self._get_reward(action)\n",
    "        next_state = self._get_observation()\n",
    "        \n",
    "        return next_state, reward, done, {}\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        return self.df.iloc[self.current_step][['cpu_usage', 'memory_usage', 'bandwidth_inbound', 'bandwidth_outbound', 'tps', 'tps_error', 'response_time']].values\n",
    "    \n",
    "    def _get_reward(self, action):\n",
    "        if action == self.df.iloc[self.current_step]['Status']:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return -1.0\n",
    "\n",
    "# %%\n",
    "# Training DQN\n",
    "env = CustomEnv()\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "dqn_agent = DQNAgent(state_dim, action_dim)\n",
    "\n",
    "num_episodes = 1000\n",
    "update_target_steps = 10\n",
    "episode_rewards = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = dqn_agent.select_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        dqn_agent.store_transition(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        dqn_agent.replay()\n",
    "\n",
    "    dqn_agent.update_target_network()\n",
    "\n",
    "    episode_rewards.append(total_reward)\n",
    "    if episode % update_target_steps == 0:\n",
    "        print(f\"Episode {episode}: Total Reward: {total_reward}\")\n",
    "\n",
    "# %%\n",
    "# Plotting rewards\n",
    "plt.plot(range(num_episodes), episode_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Training DQN')\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Evaluating DQN\n",
    "def evaluate_dqn(agent, env, n_eval_episodes=10):\n",
    "    rewards = []\n",
    "    for episode in range(n_eval_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.select_action(state)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "        rewards.append(total_reward)\n",
    "    return np.mean(rewards), np.std(rewards)\n",
    "\n",
    "mean_reward, std_reward = evaluate_dqn(dqn_agent, env)\n",
    "print(f\"Mean Reward: {mean_reward} +/- {std_reward}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
